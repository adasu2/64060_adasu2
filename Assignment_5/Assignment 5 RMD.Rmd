---
title: "Assignment 5 - Hierarchial Clustering"
author: ankith dasu
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

library(readr)
library(tidyverse)
library(cluster)
library(caret)
library(factoextra)
library(dendextend)
library(dplyr)
library(treemap)



#Loading the data unto the dataframe
```{r}
getwd()
setwd("/Users/ankithdasu/Desktop/Spring 2022/Fundamentals of Machine Learning/Assignment 5")
CerealData  <- read.csv("cereals.csv")

#Determining the structure of the data cereals
str(CerealData)
```


# Data Preparation

```{r}
#checking for Null values , there are 4 missing values
colSums(is.na(CerealData))

#removing the missing values 
CerealData <- na.omit(CerealData)

# Considering only numerical data from column 4 to 16 
CerealData <- CerealData[4:16] 
str(CerealData)
```


```{r}

#scaling the dataset(Z standards)

ScaledCereal <- as.data.frame(scale(CerealData))

```

Q1) Apply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Use Agnes to compare the clustering from single linkage, complete linkage, average linkage, and Ward. Choose the best method.

#Using Agnes method we can compare the clusting from single,complete, Average and war linkage method

```{r}

library(cluster)

complete <- agnes(ScaledCereal, method = "complete")
complete$ac # Agglomerative coefficient here is 83 %, stronger cluster structure

```


#"Single method computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the smallest of these dissimilarities as a linkage criterion. It tends to produce long clusters."

```{r}
single <- agnes(ScaledCereal, method = "single")
single$ac # Agglomerative coefficient is 60 %, weaker cluster structure. 
```

#"Average method computes all pairwise dissimilarities between the elements in cluster 1 and the elements in cluster 2, and considers the average of these dissimilarities as the distance between the two clusters"
```{r}
average <- agnes(ScaledCereal, method = "average")
average$ac #Agglomerative coefficient is 77 %, average cluster structure 
```

#Ward method minimizes the total within-cluster variance. At each step the pair of clusters with minimum between-cluster distance are merged.

```{r}
Ward  <- agnes(ScaledCereal, method = "ward")
plot(Ward)
Ward$ac # Agglomerative coefficeint is 90 %,stronger cluster structure 
```

#From the below Agnes function we notice that the Ward method has the strong cluster structure with the coefficient of 90%. This indicates that structure is stronger and the value is close to 1

```{r}
data.frame(complete$ac,single$ac,average$ac,Ward$ac)
```




Q2) How many clusters would you choose?


```{r}
HI_Cereal <- agnes(ScaledCereal,method="ward")

#visualizing the dendogram
pltree(HI_Cereal,cex = 0.7 , hang = -1 , main = "Dendrogram of Agnes using ward method")

# k value can be determined by looking at the largest difference of height, so K =5 is the optimum.
plot(HI_Cereal)

rect.hclust(HI_Cereal, k = 5, border = 1:5,)

abline(h=12,lty=12)
```


```{r}
library(dplyr)
#cutting tree
cluster_Tree = cutree(HI_Cereal,k=5)
Cereals_C <- mutate(ScaledCereal,cluster=cluster_Tree)
Cereals_C$cluster # no of cluster
```


#Partitioning the data

```{r}
library(caret)
library(tidyr)
library(factoextra)

# creating 80% partition of cluster data
set.seed(1234)
Partition <- createDataPartition(Cereals_C$cluster, p = 0.8 , list = FALSE)
A <- Cereals_C[Partition,]
B <- Cereals_C[-Partition,]

#Finding centroids for A by gathering features and values in partition and grouping is applied by cluster features and then summmarizing the mean values 

Centroid_A <- A %>% gather("features","values",-cluster) %>% group_by(cluster,features) %>% summarise(mean_values = mean(values)) %>% spread(features,mean_values) 

Centroid_A # Centroids for each cluster per column 

Cluster_B <- data.frame(data=seq(1,nrow(B),1),BClus = rep(0,nrow(B))) # Finding cluster B from the columns and no of row in B and for cluster it is repeated till the last element in last row in B is identified 

#Finding the minimum distance between centroids in A and each observation in B

#this 'for' loop will  calculate the distance between centroid and each observation of B for complete length of the observation from min vales to the max. x is the element.

for (x in 1:nrow(B)) 
  { 
  
  Cluster_B$BClus[x] <- which.min(as.matrix(get_dist(as.data.frame(rbind(Centroid_A[-1],B[x,-length(B)])))) [6,-6])
  
}

#Comparing B partition with original data
Cluster_B <- Cluster_B %>% mutate(originalcluster = B$cluster)
mean(Cluster_B$BClus == Cluster_B$originalcluster)
```

"Based on the above analysis both the original and predicted cluste matches at a percentage of 92. So here, the  cluster has a good stability but some distances might be huge, which is causing it not to be 100%. This might be due to the data partition."


Q3) 

The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.”

```{r}
Healthy_Cereals <- split(Cereals_C,Cereals_C$cluster) # splitting Cereal cluster from cereal_c data frame
Healthy_MC <-lapply(Healthy_Cereals,colMeans) # Lapply is used to apply a function over a dataframe and return the same length 
(centroids <- do.call(rbind,Healthy_MC)) # binding data frame 
heatmap(centroids) # heatmap of the centroids.
```

"Based on the above analysis, here we observe that the Bran Cerelas (Cluster 1) is recommended for children as it has high fiber, protein,potassium and low on sugars,calories and carbs."




